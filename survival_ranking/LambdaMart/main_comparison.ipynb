{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambdamart_surv import LambdaMART\n",
    "from lambdamart_cens import LambdaMARTC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_rsf(t, e):\n",
    "    \n",
    "    y = np.zeros(len(t), dtype = {'names': ('e', 't'),\n",
    "                                            'formats': ('bool', 'i4')})\n",
    "\n",
    "    y['e'] = e > 0\n",
    "    y['t'] = t\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_data(time, event, x):\n",
    "    \n",
    "    data = []\n",
    "    for i in range(len(time)):\n",
    "        new_arr = []\n",
    "        new_arr.append(int(time[i]))\n",
    "        new_arr.append(int(event[i]))\n",
    "        arr = x[i, :]\n",
    "        for el in arr:\n",
    "            new_arr.append(float(el))\n",
    "        data.append(new_arr)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6.9204912185668945 seconds ---\n",
      "Dataset:  veteran.csv\n",
      "Data shape:  (137, 9)\n",
      "Our: Test 0.7329545454545454 - Train 0.8631390134529148\n",
      "--- 7.088836193084717 seconds ---\n",
      "Dataset:  addicts.csv\n",
      "Data shape:  (238, 3)\n",
      "Our: Test 0.6055276381909548 - Train 0.6886068943706515\n",
      "--- 23.92865014076233 seconds ---\n",
      "Dataset:  lung.csv\n",
      "Data shape:  (228, 8)\n",
      "Our: Test 0.5928853754940712 - Train 0.8495363214837712\n",
      "--- 57.88116717338562 seconds ---\n",
      "Dataset:  primary_biliary_cirrhosis.csv\n",
      "Data shape:  (418, 19)\n",
      "Our: Test 0.8225729316116691 - Train 0.9233421244955673\n"
     ]
    }
   ],
   "source": [
    "files = ['veteran.csv', 'addicts.csv', 'lung.csv', 'primary_biliary_cirrhosis.csv']\n",
    "\n",
    "# files = [ 'addicts.csv', 'employee_attrition.csv', 'flchain.csv', 'gabs.csv', 'GBSG2.csv', \\\n",
    "#         'lung.csv', 'metabric.csv', 'nwtco.csv', 'primary_biliary_cirrhosis.csv', 'rotterdam.csv', \\\n",
    "#             'support.csv', 'Telco-CLT.csv', 'Telco-CLV.csv', 'veteran.csv']\n",
    "\n",
    "# files = ['GBSG2.csv']\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    path = os.getcwd()+'/../../data/'\n",
    "    file_name = path+file\n",
    "    data = pd.read_csv(file_name)\n",
    "\n",
    "    # PREPROCESS DATA\n",
    "    X = data.iloc[:, :-2]\n",
    "    \n",
    "    if file == 'veteran.csv':\n",
    "        X = pd.get_dummies(X, columns=['Celltype'])\n",
    "        nt = 300\n",
    "        lnr = .3\n",
    "    \n",
    "    if file == 'lung.csv':\n",
    "        nt = 300\n",
    "        lnr = .003\n",
    "    \n",
    "    if file == 'addicts.csv':\n",
    "        nt = 100\n",
    "        lnr = .03\n",
    "    \n",
    "    if file == 'primary_biliary_cirrhosis.csv':\n",
    "        nt = 200\n",
    "        lnr = .003\n",
    "        \n",
    "    if file == 'primary_biliary_cirrhosis.csv':\n",
    "        X = pd.get_dummies(X, columns=['sex'])\n",
    "        \n",
    "    if file == 'GBSG2.csv':\n",
    "        X = pd.get_dummies(X, columns=['horTh', 'tgrade', 'menostat'])\n",
    "        nt = 300\n",
    "        lnr = .003\n",
    "    if file == 'rotterdam.csv':\n",
    "        X = pd.get_dummies(X, columns=['size'])\n",
    "    \n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    X_normalize = preprocessing.scale(X)\n",
    "    time_all = data.iloc[:, -2].values\n",
    "    event_all = data.iloc[:, -1].values\n",
    "    \n",
    "    time_all = data.iloc[:, -2].fillna(0).round(0).astype(int)\n",
    "    event_all = data.iloc[:, -1]\n",
    "        \n",
    "    x_train, x_test, event_train, event_test = train_test_split(X_normalize, event_all,\n",
    "                                                            stratify=event_all, \n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=2436)\n",
    "\n",
    "    time_train, time_test = time_all.loc[event_train.index], time_all.loc[event_test.index]\n",
    "    \n",
    "    training_data = get_data(time_train.values, event_train.values, x_train)\n",
    "    test_data \t  = get_data(time_test.values, event_test.values, x_test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = LambdaMART(training_data, number_of_trees=nt, learning_rate=lnr, tree_type='sklearn')\n",
    "    model.fit()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    perf_our = concordance_index_censored(event_test.astype(bool), time_test, model.predict(test_data))[0]\n",
    "    perf_our_tr = concordance_index_censored(event_train.astype(bool), time_train, model.predict(training_data))[0]\n",
    "\n",
    "    print('Dataset: ', file)\n",
    "    print('Data shape: ', X.shape)\n",
    "    print('Our: ' + 'Test ' + str(perf_our) + ' - Train ' + str(perf_our_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = [ 'addicts.csv', 'employee_attrition.csv', 'flchain.csv', 'gabs.csv', 'GBSG2.csv', \\\n",
    "        'lung.csv', 'metabric.csv', 'nwtco.csv', 'primary_biliary_cirrhosis.csv', 'rotterdam.csv', \\\n",
    "            'support.csv', 'Telco-CLT.csv', 'Telco-CLV.csv', 'veteran.csv']\n",
    "data_size = [238, 15000, 6525, 2233, 687, 229, 1905, 4029, 419, 2983, 8874, 7044, 7044, 138]\n",
    "elapsed_time = [4.04, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lambdamart():\n",
    "    \n",
    "    # files = ['veteran.csv', 'addicts.csv', 'lung.csv', 'primary_biliary_cirrhosis.csv']\n",
    "    files = ['gabs.csv']\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        print('FILE: ', file)\n",
    "        # GET DATA\n",
    "        path = os.getcwd()+'/../../data/'\n",
    "        file_name = path+file\n",
    "        data = pd.read_csv(file_name)\n",
    "        \n",
    "        \n",
    "        # PREPROCESS DATA\n",
    "        X = data.iloc[:, :-2]\n",
    "        \n",
    "        if file == 'veteran.csv':\n",
    "            X = pd.get_dummies(X, columns=['Celltype'])\n",
    "            \n",
    "        if file == 'primary_biliary_cirrhosis.csv':\n",
    "            X = pd.get_dummies(X, columns=['sex'])\n",
    "            \n",
    "        if file == 'GBSG2.csv':\n",
    "            X = pd.get_dummies(X, columns=['horTh', 'tgrade', 'menostat'])\n",
    "            \n",
    "        if file == 'rotterdam.csv':\n",
    "            X = pd.get_dummies(X, columns=['size'])\n",
    "        \n",
    "        \n",
    "        X= X.fillna(X.median())\n",
    "        \n",
    "        X_normalize = preprocessing.scale(X)\n",
    "\n",
    "        time_all = data.iloc[:, -2].fillna(0).round(0).astype(int)\n",
    "        event_all = data.iloc[:, -1]\n",
    "        \n",
    "        x_train1, x_test, event_train1, event_test = train_test_split(X_normalize, event_all,\n",
    "                                                            stratify=event_all, \n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=2436)\n",
    "\n",
    "        time_train1, time_test = time_all.loc[event_train1.index], time_all.loc[event_test.index]\n",
    "        \n",
    "        x_train, x_val, event_train, event_val = train_test_split(x_train1, event_train1,\n",
    "                                                                stratify=event_train1, \n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=2436)\n",
    "\n",
    "        time_train, time_val = time_train1.loc[event_train.index], time_train1.loc[event_val.index]\n",
    "        \n",
    "        training_data = get_data(time_train.values, event_train.values, x_train)\n",
    "        val_data = get_data(time_val.values, event_val.values, x_val)\n",
    "        test_data \t  = get_data(time_test.values, event_test.values, x_test)\n",
    "        \n",
    "        s_lambda       = np.zeros((9, ))\n",
    "        lrs         = [0.003, 0.03, 0.3]\n",
    "        nest        = [100, 200, 300]\n",
    "        \n",
    "        \n",
    "        i = 0\n",
    "        for lr in lrs:\n",
    "            for nes in nest: \n",
    "                model = LambdaMART(training_data, number_of_trees=nes, learning_rate=lr, tree_type='sklearn')\n",
    "                model.fit()\n",
    "                s_lambda[i] = concordance_index_censored(event_val.astype(bool), time_val, model.predict(val_data))[0]\n",
    "                \n",
    "                print('lr: ' + str(lr) + ', nest: '+str(nes)+' score: '+str(s_lambda[i]))\n",
    "                \n",
    "                i += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE:  gabs.csv\n"
     ]
    }
   ],
   "source": [
    "tune_lambdamart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
